{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节中，我们介绍了如何用密集连接的神经网络将向量输入划分为两个互斥的类别，本节你会构建一个网络，将路透社新闻划分为46 个互斥的主题。因为有多个类别，所以 这是多分类（multiclass classification）问题\n",
    "- 因为每个数据点只能划分到一个类别， 所以更具体地说，这是单标签、多分类（single-label, multiclass classification）问题\n",
    "- 每个数据点可以划分到多个类别（主题），那它就是一个多标签、多分类（multilabel, multiclass classification）问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 路透社数据集\n",
    "包含许多短新闻及其对应的主题，由路透社在1986 年发布。它 是一个简单的、广泛使用的文本分类数据集。它包括46 个不同的主题：某些主题的样本更多， 但训练集中每个主题都有至少 10 个样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data,train_labels),(test_data,test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 8982 2246 2246\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(train_labels),len(test_data),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]===8982\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0],len(train_data),sep='===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "45 0\n"
     ]
    }
   ],
   "source": [
    "# 样本对应的标签是一个 0~45 范围内的整数，即话题索引编号\n",
    "print(train_labels[0])\n",
    "print(max(train_labels),min(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解码索引为文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? the u s agriculture department estimated canada's 1986 87 wheat crop at 31 85 mln tonnes vs 31 85 mln tonnes last month it estimated 1985 86 output at 24 25 mln tonnes vs 24 25 mln last month canadian 1986 87 coarse grain production is projected at 27 62 mln tonnes vs 27 62 mln tonnes last month production in 1985 86 is estimated at 24 95 mln tonnes vs 24 95 mln last month canadian wheat exports in 1986 87 are forecast at 19 00 mln tonnes vs 18 00 mln tonnes last month exports in 1985 86 are estimated at 17 71 mln tonnes vs 17 72 mln last month reuter 3\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "index_word = dict([(value,key) for (key,value) in word_index.items()])\n",
    "# 解码某一条新闻序列,记得i-3\n",
    "# 因为0、1、2 是为“padding”（填充）、“ start of sequence”、“unknown”分别保留的索引\n",
    "decoded_newswire = ' '.join(index_word.get(i-3,'?') for i in train_data[5])\n",
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据\n",
    "#### 将数据向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences,dimension=10000):\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    for i,sequence in enumerate(sequences):\n",
    "        results[i,sequence] = 1\n",
    "    return results\n",
    "\n",
    "# 向量化训练和测试数据\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将标签向量化\n",
    "标签向量化有两种方法：你可以将标签列表转换为整数张量，或者使用one-hot 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot 编码\n",
    "def to_one_hot(labels,dimension=46):\n",
    "    results = np.zeros((len(labels),dimension))\n",
    "    for i,label in enumerate(labels):\n",
    "        results[i,label] = 1\n",
    "    return results\n",
    "\n",
    "# one_hot 训练和测试标签\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_hot_train_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keras 有内置方法可以实现这个操作\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "网络的最后一层是大小为46 的 Dense 层。这意味着，对于每个输入样本，网络都会输 出一个 46 维向量。这个向量的每个元素（即每个维度）代表不同的输出类别\n",
    "\n",
    "最后一层使用了 softmax 激活。网络将输出在46 个不同输出类别上的概率分布——对于每一个输入样本，网络都会输出一个 46 维向量， 其中 output[i] 是样本属于第 i 个类别的概率。46 个概率的总和为 1。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编译模型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "对于这个例子，最好的损失函数是 categorical_crossentropy（分类交叉熵）。它用于 衡量两个概率分布之间的距离，这里两个概率分布分别是网络输出的概率分布和标签的真实分 布。通过将这两个分布的距离最小化，训练网络可使输出结果尽可能接近真实标签。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分开验证和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 191us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7204 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 149us/step - loss: 1.4450 - acc: 0.6878 - val_loss: 1.3457 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 146us/step - loss: 1.0951 - acc: 0.7648 - val_loss: 1.1704 - val_acc: 0.7420\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 150us/step - loss: 0.8695 - acc: 0.8161 - val_loss: 1.0795 - val_acc: 0.7590\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 138us/step - loss: 0.7032 - acc: 0.8480 - val_loss: 0.9846 - val_acc: 0.7820\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 142us/step - loss: 0.5665 - acc: 0.8795 - val_loss: 0.9409 - val_acc: 0.8030\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 145us/step - loss: 0.4580 - acc: 0.9049 - val_loss: 0.9074 - val_acc: 0.8010\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 148us/step - loss: 0.3696 - acc: 0.9228 - val_loss: 0.9349 - val_acc: 0.7900\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 144us/step - loss: 0.3032 - acc: 0.9311 - val_loss: 0.8917 - val_acc: 0.8090\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 148us/step - loss: 0.2538 - acc: 0.9412 - val_loss: 0.9065 - val_acc: 0.8120\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 143us/step - loss: 0.2183 - acc: 0.9470 - val_loss: 0.9177 - val_acc: 0.8130\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 141us/step - loss: 0.1872 - acc: 0.9509 - val_loss: 0.9022 - val_acc: 0.8160\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 147us/step - loss: 0.1698 - acc: 0.9525 - val_loss: 0.9341 - val_acc: 0.8090\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 141us/step - loss: 0.1531 - acc: 0.9557 - val_loss: 0.9720 - val_acc: 0.8060\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 139us/step - loss: 0.1390 - acc: 0.9557 - val_loss: 0.9691 - val_acc: 0.8140\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 136us/step - loss: 0.1312 - acc: 0.9560 - val_loss: 1.0251 - val_acc: 0.8050\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 137us/step - loss: 0.1215 - acc: 0.9579 - val_loss: 1.0256 - val_acc: 0.7960\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 137us/step - loss: 0.1198 - acc: 0.9577 - val_loss: 1.0436 - val_acc: 0.8080\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 138us/step - loss: 0.1137 - acc: 0.9594 - val_loss: 1.0935 - val_acc: 0.7970\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 142us/step - loss: 0.1108 - acc: 0.9598 - val_loss: 1.0656 - val_acc: 0.8010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=20,\n",
    "                    validation_data=[x_val,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
