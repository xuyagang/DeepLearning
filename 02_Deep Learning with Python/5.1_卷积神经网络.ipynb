{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minist 数据集回顾\n",
    "是机器学习领域的一个经典数据集，其历史几乎和这 个领域一样长，而且已被人们深入研究\n",
    "#### 经典6+1（万）\n",
    "这个数据集包含 60 000 张训练图像和 10 000 张测试图像\n",
    "####\n",
    "图像被编码为 Numpy 数组，而标签是数字数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51 159 253\n",
      "  159  50   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 238 252 252\n",
      "  252 237   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  54 227 253 252 239\n",
      "  233 252  57   6   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202\n",
      "   84 252 253 122   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 163 252 252 252 253 252 252\n",
      "   96 189 253 167   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "   47  79 255 168   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  48 238 252 252 179  12  75 121  21\n",
      "    0   0 253 243  50   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0   0   0\n",
      "    0   0 253 252 165   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   7 178 252 240  71  19  28   0   0   0   0\n",
      "    0   0 253 252 195   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  57 252 252  63   0   0   0   0   0   0   0\n",
      "    0   0 253 252 195   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 198 253 190   0   0   0   0   0   0   0   0\n",
      "    0   0 255 253 196   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  76 246 252 112   0   0   0   0   0   0   0   0\n",
      "    0   0 253 252 148   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0\n",
      "    7 135 253 186  12   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 223   0   0   0   0   0   0   0   0   7\n",
      "  131 252 225  71   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      "  252 173   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  86 253 225   0   0   0   0   0   0 114 238 253\n",
      "  162   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253 223 167\n",
      "   56   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 252 252 229 215 252 252 252 196 130   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  28 199 252 252 253 252 252 233 145   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  25 128 252 253 252 141  37   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "0\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n",
    "\n",
    "print(train_images[1])\n",
    "print(train_labels[1])\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[1],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的卷积神经网络。它是 Conv2D 层和 MaxPooling2D 层的堆叠\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "# 卷积神经网络接收形状为 (image_height, image_width, image_channels) 的输入张量\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))  \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))  \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# 第一个卷积层接收一个大小为 (28, 28, 1) 的特征图，并输出一个大 小为 (26, 26, 32) 的特征图,\n",
    "# 即它在输入上计算 32 个过滤器,每个 通道都包含一个 26×26 的数值网格，它是过滤器对输入的响应图（response map）,\n",
    "# 表示这个过 滤器模式在输入中不同位置的响应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个 Conv2D 层和 MaxPooling2D 层的输出都是一个形状为 (height, width, channels) 的3D 张量\n",
    "\n",
    "宽度和高度两个维度的尺寸通常会随着网络加深而变小\n",
    "\n",
    "通道数量由传 入 Conv2D 层的第一个参数所控制（32 或64）\n",
    "\n",
    "下一步是将最后的输出张量［大小为 (3, 3, 64)］输入到一个密集连接分类器网络中， 即 Dense 层的堆叠\n",
    "\n",
    "这些分类器可以处理 1D 向量，而当前的输出是 3D 张量。 首先，我们需要将 3D 输出展平为 1D，然后在上面添加几个 Dense 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(layers.Flatten()) \n",
    "model.add(layers.Dense(64, activation='relu')) \n",
    "model.add(layers.Dense(10, activation='softmax')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进入两个 Dense 层之前，形状 (3, 3, 64) 的输出被展平为形状 (576,) 的 向量\n",
    "\n",
    "下面我们在MNIST 数字图像上训练这个卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 32s 528us/step - loss: 0.1753 - acc: 0.9457\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 32s 530us/step - loss: 0.0474 - acc: 0.9853\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 31s 521us/step - loss: 0.0330 - acc: 0.9900\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 32s 534us/step - loss: 0.0249 - acc: 0.9923\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 32s 533us/step - loss: 0.0194 - acc: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1df04498978>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28,1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28,1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_images,train_labels,epochs=5,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 186us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9916"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在测试数据上对模型进行评估\n",
    "test_loss,test_acc = model.evaluate(test_images,test_labels)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
